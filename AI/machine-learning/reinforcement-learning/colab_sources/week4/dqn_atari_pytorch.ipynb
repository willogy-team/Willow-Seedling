{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_atari_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-l_WrZISIPxn",
        "OiW2GO16IPxp",
        "oXBYpPZRIPxz",
        "u-3XJJYyIPx1",
        "2d1kjmhpIPx2",
        "nM3wlsXNIPx4",
        "-UPxQTb_IPx4",
        "QXUVUhwpIPx_",
        "fDMYbgFrIPyB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx22TJHoIPxa"
      },
      "source": [
        "# Deep Q-Network implementation.\n",
        "\n",
        "This homework shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way.\n",
        "\n",
        "Original paper:\n",
        "https://arxiv.org/pdf/1312.5602.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1CsMjFiIPxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cfaca64-29fa-4d9d-99dd-8606b775b8d7"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    \n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week4_approx/submit.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week4_approx/framebuffer.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week4_approx/replay_buffer.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week4_approx/atari_wrappers.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week4_approx/utils.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN1jcz-2IPxf"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for pytoch, but you find it easy to adapt it to almost any python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUxMXB7vIPxf"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HlLVtzJIPxg"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulRdnhttIPxg"
      },
      "source": [
        "### Let's play some old videogames\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/nerd.png)\n",
        "\n",
        "This time we're gonna apply approximate q-learning to an Atari game called Breakout. It's not the hardest thing out there, but it's definitely way more complex than anything we tried before.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewrhz2vjIPxh"
      },
      "source": [
        "ENV_NAME = \"BreakoutNoFrameskip-v4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW15LGCBIPxi"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fR4xxNDIPxi"
      },
      "source": [
        "Let's see what observations look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeW2dJoyIPxj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "8b5ee369-36d3-475c-986e-bdbb56a4c059"
      },
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "env.reset()\n",
        "\n",
        "n_cols = 5\n",
        "n_rows = 2\n",
        "fig = plt.figure(figsize=(16, 9))\n",
        "\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)\n",
        "        ax.imshow(env.render('rgb_array'))\n",
        "        env.step(env.action_space.sample())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x648 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZW7XsxJIPxk"
      },
      "source": [
        "**Let's play a little.**\n",
        "\n",
        "Pay attention to zoom and fps args of play function. Control: A, D, space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db40q-b_IPxm"
      },
      "source": [
        "# # does not work in Colab.\n",
        "# # make keyboard interrupt to continue\n",
        "\n",
        "# from gym.utils.play import play\n",
        "\n",
        "# play(env=gym.make(ENV_NAME), zoom=5, fps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jyBq99vIPxm"
      },
      "source": [
        "### Processing game image \n",
        "\n",
        "Raw Atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n",
        "\n",
        "We can thus save a lot of time by preprocessing game image, including\n",
        "* Resizing to a smaller shape, 64 x 64\n",
        "* Converting to grayscale\n",
        "* Cropping irrelevant image parts (top, bottom and edges)\n",
        "\n",
        "Also please keep one dimension for channel so that final shape would be 1 x 64 x 64.\n",
        "\n",
        "Tip: You can implement your own grayscale converter and assign a huge weight to the red channel. This dirty trick is not necessary but it will speed up learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErQV2wyZIPxm"
      },
      "source": [
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces import Box\n",
        "import cv2\n",
        "import copy\n",
        "\n",
        "\n",
        "class PreprocessAtariObs(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and grayscales it.\"\"\"\n",
        "        ObservationWrapper.__init__(self, env)\n",
        "\n",
        "        self.img_size = (1, 64, 64)\n",
        "        self.observation_space = Box(0.0, 1.0, self.img_size)\n",
        "\n",
        "\n",
        "    def _to_gray_scale(self, rgb, channel_weights=[0.8, 0.1, 0.1]):\n",
        "        # <YOUR CODE>\n",
        "        H, W, c = rgb.shape\n",
        "\n",
        "        gray_scale_img = np.zeros((1, H, W), dtype='float32')\n",
        "        for x in range(H):\n",
        "            for y in range(W):\n",
        "                gray_scale_img[0][x][y] = np.array(sum(rgb[x][y] * np.array(channel_weights)))\n",
        "        return gray_scale_img\n",
        "\n",
        "\n",
        "    def observation(self, img):\n",
        "        \"\"\"what happens to each observation\"\"\"\n",
        "       \n",
        "        # Here's what you need to do:\n",
        "        #  * crop image, remove irrelevant parts\n",
        "        #  * resize image to self.img_size\n",
        "        #     (use imresize from any library you want,\n",
        "        #      e.g. opencv, skimage, PIL, keras)\n",
        "        #  * cast image to grayscale\n",
        "        #  * convert image pixels to (0,1) range, float32 type\n",
        "        # <YOUR CODE>\n",
        "\n",
        "        H, W, C = img.shape\n",
        "        if C ==3:\n",
        "    \n",
        "            result_img = cv2.resize(img, None,fx=(64/W), fy=(64/H), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "            result_img = self._to_gray_scale(result_img)\n",
        "            \n",
        "            result_img /= 255.0\n",
        "            \n",
        "        \n",
        "        return result_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6W8WxlvIPxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "9287e366-983e-4b95-fd89-580d8eea31b0"
      },
      "source": [
        "import gym\n",
        "# spawn game instance for tests\n",
        "env = gym.make(ENV_NAME)  # create raw env\n",
        "env = PreprocessAtariObs(env)\n",
        "observation_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "env.reset()\n",
        "obs, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "# test observation\n",
        "assert obs.ndim == 3, \"observation must be [channel, h, w] even if there's just one channel\"\n",
        "print(obs.shape , observation_shape)\n",
        "assert obs.shape == observation_shape\n",
        "assert obs.dtype == 'float32'\n",
        "assert len(np.unique(obs)) > 2, \"your image must not be binary\"\n",
        "assert 0 <= np.min(obs) and np.max(\n",
        "    obs) <= 1, \"convert image pixels to [0,1] range\"\n",
        "\n",
        "assert np.max(obs) >= 0.5, \"It would be easier to see a brighter observation\"\n",
        "assert np.mean(obs) >= 0.1, \"It would be easier to see a brighter observation\"\n",
        "\n",
        "print(\"Formal tests seem fine. Here's an example of what you'll get.\")\n",
        "\n",
        "n_cols = 5\n",
        "n_rows = 2\n",
        "fig = plt.figure(figsize=(16, 9))\n",
        "obs = env.reset()\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)\n",
        "        ax.imshow(obs[0, :, :], interpolation='none', cmap='gray')\n",
        "        obs, _, _, _ = env.step(env.action_space.sample())\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 64, 64) (1, 64, 64)\n",
            "Formal tests seem fine. Here's an example of what you'll get.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x648 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l_WrZISIPxn"
      },
      "source": [
        "### Wrapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rdoIvBMIPxn"
      },
      "source": [
        "**About the game:** You have 5 lives and get points for breaking the wall. Higher bricks cost more than the lower ones. There are 4 actions: start game (should be called at the beginning and after each life is lost), move left, move right and do nothing. There are some common wrappers used for Atari environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gGR22mwIPxo"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import atari_wrappers\n",
        "\n",
        "def PrimaryAtariWrap(env, clip_rewards=True):\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "\n",
        "    # This wrapper holds the same action for <skip> frames and outputs\n",
        "    # the maximal pixel value of 2 last frames (to handle blinking\n",
        "    # in some envs)\n",
        "    env = atari_wrappers.MaxAndSkipEnv(env, skip=4)\n",
        "\n",
        "    # This wrapper sends done=True when each life is lost\n",
        "    # (not all the 5 lives that are givern by the game rules).\n",
        "    # It should make easier for the agent to understand that losing is bad.\n",
        "    env = atari_wrappers.EpisodicLifeEnv(env)\n",
        "\n",
        "    # This wrapper laucnhes the ball when an episode starts.\n",
        "    # Without it the agent has to learn this action, too.\n",
        "    # Actually it can but learning would take longer.\n",
        "    env = atari_wrappers.FireResetEnv(env)\n",
        "\n",
        "    # This wrapper transforms rewards to {-1, 0, 1} according to their sign\n",
        "    if clip_rewards:\n",
        "        env = atari_wrappers.ClipRewardEnv(env)\n",
        "\n",
        "    # This wrapper is yours :)\n",
        "    env = PreprocessAtariObs(env)\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF9QUWCBIPxo"
      },
      "source": [
        "**Let's see if the game is still playable after applying the wrappers.**\n",
        "At playing the EpisodicLifeEnv wrapper seems not to work but actually it does (because after when life finishes a new ball is dropped automatically - it means that FireResetEnv wrapper understands that a new episode began)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBOVL7r9IPxo"
      },
      "source": [
        "# # does not work in Colab.\n",
        "# # make keyboard interrupt to continue\n",
        "\n",
        "# from gym.utils.play import play\n",
        "\n",
        "# def make_play_env():\n",
        "#     env = gym.make(ENV_NAME)\n",
        "#     env = PrimaryAtariWrap(env)\n",
        "# # in torch imgs have shape [c, h, w] instead of common [h, w, c]\n",
        "#     env = atari_wrappers.AntiTorchWrapper(env)\n",
        "#     return env\n",
        "\n",
        "# play(make_play_env(), zoom=10, fps=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiW2GO16IPxp"
      },
      "source": [
        "### Frame buffer\n",
        "\n",
        "Our agent can only process one observation at a time, so we gotta make sure it contains enough information to find optimal actions. For instance, agent has to react to moving objects so he must be able to measure object's velocity.\n",
        "\n",
        "To do so, we introduce a buffer that stores 4 last images. This time everything is pre-implemented for you, not really by the staff of the course :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKXIGezZIPxx"
      },
      "source": [
        "from framebuffer import FrameBuffer\n",
        "\n",
        "def make_env(clip_rewards=True, seed=None):\n",
        "    env = gym.make(ENV_NAME)  # create raw env\n",
        "    if seed is not None:\n",
        "        env.seed(seed)\n",
        "    env = PrimaryAtariWrap(env, clip_rewards)\n",
        "    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_shape = env.observation_space.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeNNhzBSIPxy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30745e1b-9283-4e9a-8bf7-c3337e5eaef9"
      },
      "source": [
        "for _ in range(12):\n",
        "    obs, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "plt.figure(figsize=[12,10])\n",
        "plt.title(\"Game image\")\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=[15,15])\n",
        "plt.title(\"Agent observation (4 frames top to bottom)\")\n",
        "plt.imshow(utils.img_by_obs(obs, state_shape), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-IGHNqJIPxz"
      },
      "source": [
        "## DQN as it is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXBYpPZRIPxz"
      },
      "source": [
        "### Building a network\n",
        "\n",
        "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n",
        "\n",
        "You can build any architecture you want, but for reference, here's something that will more or less work:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL6OD4DvIPxz"
      },
      "source": [
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/dqn_arch.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxNYmfpkIPxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82dba772-9406-47e9-8533-96f64423340d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# those who have a GPU but feel unfair to use it can uncomment:\n",
        "# device = torch.device('cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZF3WqWSIPx0"
      },
      "source": [
        "def conv2d_size_out(size, kernel_size, stride):\n",
        "    \"\"\"\n",
        "    common use case:\n",
        "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
        "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
        "    to understand the shape for dense layer's input\n",
        "    \"\"\"\n",
        "    return (size - (kernel_size - 1) - 1) // stride  + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADY3OcStIPx0"
      },
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "\n",
        "        # Define your network body here. Please make sure agent is fully contained here\n",
        "        # nn.Flatten() can be useful\n",
        "        # <YOUR CODE>\n",
        "        self.network = nn.Sequential()\n",
        "        self.network.add_module('conv1', nn.Conv2d(4, 16, kernel_size=3, stride=2, padding=1))\n",
        "        self.network.add_module('relu', nn.ReLU())\n",
        "\n",
        "        self.network.add_module('conv2', nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1))\n",
        "        #self.network.add_module('relu2', nn.ReLU())\n",
        "\n",
        "        self.network.add_module('conv3', nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1))\n",
        "        #self.network.add_module('relu3', nn.ReLU())\n",
        "        \n",
        "        self.network.add_module('flat', nn.Flatten(1) )\n",
        "        self.network.add_module('dense', nn.Linear(64*8*8, 256))\n",
        "        #self.network.add_module('reluD', nn.ReLU())\n",
        "\n",
        "        self.network.add_module('output', nn.Linear(256, self.n_actions))\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        \"\"\"\n",
        "        takes agent's observation (tensor), returns qvalues (tensor)\n",
        "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
        "        \"\"\"\n",
        "\n",
        "        # Use your network to compute qvalues for given state\n",
        "        # [batch_size, 4, h, w]\n",
        "        x = self.network.conv1(state_t)\n",
        "        x = self.network.relu(x)\n",
        "        # [batch_size, 16, h/2, w/2]\n",
        "        x = self.network.conv2(x)\n",
        "        x = self.network.relu(x)\n",
        "        # [batch_size, 32, h/4, w/4]\n",
        "        x = self.network.conv3(x)\n",
        "        x = self.network.relu(x)\n",
        "        # [batch_size, 64, h/8, w/8]\n",
        "        x = self.network.flat(x)\n",
        "        # [batch_size, 64*h/8*w/8]\n",
        "        x = self.network.dense(x)\n",
        "        x = self.network.relu(x)\n",
        "\n",
        "        qvalues = self.network.output(x) # <YOUR CODE>\n",
        "\n",
        "\n",
        "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "        assert len(\n",
        "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, states):\n",
        "        \"\"\"\n",
        "        like forward, but works on numpy arrays, not tensors\n",
        "        \"\"\"\n",
        "        model_device = next(self.parameters()).device\n",
        "        states = torch.tensor(states, device=model_device, dtype=torch.float)\n",
        "        qvalues = self.forward(states)\n",
        "        return qvalues.data.cpu().numpy()\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "\n",
        "        should_explore = np.random.choice(\n",
        "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2KCjfgzemOM",
        "outputId": "8dc83306-f14c-416c-f1f7-3688eb462359"
      },
      "source": [
        "m = nn.Linear(64, 256)\n",
        "print(m)\n",
        "input = torch.randn(2, 64,8,8)\n",
        "print(input.shape)\n",
        "output = torch.flatten(input,1)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=64, out_features=256, bias=True)\n",
            "torch.Size([2, 64, 8, 8])\n",
            "torch.Size([2, 4096])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvDkB2fxIPx0"
      },
      "source": [
        "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hR4YtkiIPx1"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNbYXUN3IPx1"
      },
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
        "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s = env.reset()\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, done, _ = env.step(action)\n",
        "            reward += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evzg1bWvIPx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab2f613-a896-4cde-851a-9b2d5023281c"
      },
      "source": [
        "evaluate(env, agent, n_games=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3XJJYyIPx1"
      },
      "source": [
        "### Experience replay\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d1kjmhpIPx2"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg8PKFHNIPx2"
      },
      "source": [
        "from replay_buffer import ReplayBuffer\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset(), env.action_space.sample(),\n",
        "                   1.0, env.reset(), done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
        "    5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYhCj8mWIPx2"
      },
      "source": [
        "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
        "    Whenever game ends, add record with done=True and reset the game.\n",
        "    It is guaranteed that env has done=False when passed to this function.\n",
        "\n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "\n",
        "    :returns: return sum of rewards over time and the state in which the env stays\n",
        "    \"\"\"\n",
        "    s = initial_state\n",
        "    sum_rewards = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    # <YOUR CODE>\n",
        "    for i in range(n_steps):\n",
        "        q_values = agent.get_qvalues([s])\n",
        "        a = q_values.argmax(axis=-1)[0]\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        exp_replay.add(s, a, r, next_s, done)\n",
        "        sum_rewards += r\n",
        "        s = next_s\n",
        "        if done:\n",
        "          env.reset()\n",
        "\n",
        "    return sum_rewards, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7EFHGTYIPx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95df349b-eea6-4979-d452-6c2c9f84a376"
      },
      "source": [
        "# testing your code.\n",
        "exp_replay = ReplayBuffer(2000)\n",
        "\n",
        "state = env.reset()\n",
        "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \"\\\n",
        "                                 \"but instead added %i\" % len(exp_replay)\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.1, \"Please make sure you restart the game whenever it is 'done' and record the is_done correctly into the buffer.\"\\\n",
        "                                    \"Got %f is_done rate over %i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
        "                                        np.mean(is_dones), len(exp_replay))\n",
        "\n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
        "        10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
        "    assert act_batch.shape == (\n",
        "        10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
        "    assert reward_batch.shape == (\n",
        "        10,), \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
        "    assert is_done_batch.shape == (\n",
        "        10,), \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
        "    assert [int(i) in (0, 1)\n",
        "            for i in is_dones], \"is_done should be strictly True or False\"\n",
        "    assert [\n",
        "        0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
        "\n",
        "print(\"Well done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM3wlsXNIPx4"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHnaDoxZIPx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0fa096e-9833-4c2e-c186-3ac36e2d4320"
      },
      "source": [
        "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
        "# This is how you can load weights from agent into target network\n",
        "target_network.load_state_dict(agent.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UPxQTb_IPx4"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6wPisFqIPx4"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "\n",
        "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
        "\n",
        "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgT5HeQmIPx4"
      },
      "source": [
        "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
        "                    agent, target_network,\n",
        "                    gamma=0.99,\n",
        "                    check_shapes=False,\n",
        "                    device=device):\n",
        "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n",
        "\n",
        "    # for some torch reason should not make actions a tensor\n",
        "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n",
        "    # shape: [batch_size, *state_shape]\n",
        "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
        "    is_done = torch.tensor(\n",
        "        is_done.astype('float32'),\n",
        "        device=device,\n",
        "        dtype=torch.float\n",
        "    )  # shape: [batch_size]\n",
        "    is_not_done = 1 - is_done\n",
        "\n",
        "    # get q-values for all actions in current states\n",
        "    predicted_qvalues = agent(states)\n",
        "\n",
        "    # compute q-values for all actions in next states\n",
        "    predicted_next_qvalues = target_network(next_states)\n",
        "    \n",
        "    # select q-values for chosen actions\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
        "        len(actions)), actions]\n",
        "\n",
        "    # compute V*(next_states) using predicted next q-values\n",
        "    next_state_values, _ = torch.max(predicted_next_qvalues, 1)  # <YOUR CODE>\n",
        "\n",
        "    assert next_state_values.dim(\n",
        "    ) == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n",
        "\n",
        "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "    # you can multiply next state values by is_not_done to achieve this.\n",
        "    target_qvalues_for_actions = rewards + gamma * next_state_values  # <YOUR CODE>\n",
        "\n",
        "    # mean squared error loss to minimize\n",
        "    loss = torch.mean((predicted_qvalues_for_actions -\n",
        "                       target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim(\n",
        "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
        "        assert next_state_values.data.dim(\n",
        "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        assert target_qvalues_for_actions.data.dim(\n",
        "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdVxZ75nIPx6"
      },
      "source": [
        "Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB2iUzUtIPx6"
      },
      "source": [
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
        "    10)\n",
        "\n",
        "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
        "                       agent, target_network,\n",
        "                       gamma=0.99, check_shapes=True)\n",
        "loss.backward()\n",
        "\n",
        "assert loss.requires_grad and tuple(loss.data.size()) == (\n",
        "    ), \"you must return scalar loss - mean over batch\"\n",
        "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() !=\n",
        "              0), \"loss must be differentiable w.r.t. network weights\"\n",
        "assert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afxi8CG9IPx6"
      },
      "source": [
        "## Main loop\n",
        "\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-4LURh7IPx7"
      },
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyPGSmopIPx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdcaee6-b508-477c-87db-19621658a0c1"
      },
      "source": [
        "seed = 37  # <YOUR CODE: your favourite random seed>\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb6d59afb88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TshT1CYIPx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa50bfa-2d51-4508-a599-1a5d679e8324"
      },
      "source": [
        "env = make_env(seed)\n",
        "state_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "state = env.reset()\n",
        "\n",
        "agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
        "target_network = DQNAgent(state_shape, n_actions).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wLutyYIIPx8"
      },
      "source": [
        "Buffer of size $10^4$ fits into 5 Gb RAM.\n",
        "\n",
        "Larger sizes ($10^5$ and $10^6$ are common) can be used. It can improve the learning, but $10^4$ is quiet enough. $10^2$ will probably fail learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2a6lcQpIPx9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "ac25b65e-27fa-43e2-bb91-a23fa25b19b3"
      },
      "source": [
        "exp_replay = ReplayBuffer(10**4)\n",
        "for i in range(100):\n",
        "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
        "        print(\"\"\"\n",
        "            Less than 100 Mb RAM available. \n",
        "            Make sure the buffer size in not too huge.\n",
        "            Also check, maybe other processes consume RAM heavily.\n",
        "            \"\"\"\n",
        "             )\n",
        "        break\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    if len(exp_replay) == 10**4:\n",
        "        break\n",
        "print(len(exp_replay))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f02523a306e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m              )\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mplay_and_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_replay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-e5cfb39b2d3b>\u001b[0m in \u001b[0;36mplay_and_record\u001b[0;34m(initial_state, agent, env, exp_replay, n_steps)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msum_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/framebuffer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m\"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnew_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframebuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k7F6cZ2IPx9"
      },
      "source": [
        "timesteps_per_epoch = 1\n",
        "batch_size = 16\n",
        "total_steps = 3 * 10**6\n",
        "decay_steps = 10**6\n",
        "\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "init_epsilon = 1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "loss_freq = 50\n",
        "refresh_target_network_freq = 5000\n",
        "eval_freq = 5000\n",
        "save_freq = 3 * 10**5\n",
        "\n",
        "max_grad_norm = 50\n",
        "\n",
        "n_lives = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N2PLLVJa5ZF",
        "outputId": "9f4b50b8-ac05-431b-cca7-0bdab99a0b07"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0alZYBl3IPx9"
      },
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "grad_norm_history = []\n",
        "initial_state_v_history = []\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVqBLujrbHiX"
      },
      "source": [
        "PATH = f\"/content/gdrive/MyDrive/RL_Dat_Vi/dqn_atari_pytorch/dqn_step_{step}.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIY6c4s97zEw",
        "outputId": "80119a10-9e2e-4ec8-e01a-efe1f26811c4"
      },
      "source": [
        "checkpoint = torch.load(PATH)\n",
        "agent.load_state_dict(checkpoint['agent_state_dict'])\n",
        "opt.load_state_dict(checkpoint['opt_state_dict'])\n",
        "env = checkpoint['env']\n",
        "step = checkpoint['step'] + 1\n",
        "exp_replay = checkpoint[\"exp_replay\"]\n",
        "mean_rw_history = checkpoint[\"mean_rw_history\"]\n",
        "td_loss_history = checkpoint[\"td_loss_history\"]\n",
        "grad_norm_history = checkpoint[\"grad_norm_history\"]\n",
        "initial_state_v_history = checkpoint[\"initial_state_v_history\"]\n",
        "target_network = DQNAgent(state_shape, n_actions).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeBlIscQAljn",
        "outputId": "99dbb099-ce1a-4ac9-db3c-500b747accd0"
      },
      "source": [
        "step"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1200001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z3NX_mWIPx9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "dc2a73c8-4575-4a0b-c40b-95850c94a60f"
      },
      "source": [
        "state = env.reset()\n",
        "for step in trange(step, total_steps + 1):\n",
        "\n",
        "    if not utils.is_enough_ram():\n",
        "        print('less that 100 Mb RAM available, freezing')\n",
        "        print('make sure everythin is ok and make KeyboardInterrupt to continue')\n",
        "        try:\n",
        "            while True:\n",
        "                pass\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n",
        "\n",
        "    agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
        "\n",
        "    # play\n",
        "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
        "\n",
        "    # train\n",
        "    # <YOUR CODE: sample batch_size of data from experience replay>\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
        "    # <YOUR CODE: compute TD loss>\n",
        "    loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
        "                       agent, target_network,\n",
        "                       gamma=0.99, check_shapes=True)\n",
        "                        \n",
        "\n",
        "    loss.backward()\n",
        "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    if step % loss_freq == 0:\n",
        "        td_loss_history.append(loss.data.cpu().item())\n",
        "        grad_norm_history.append(grad_norm)\n",
        "\n",
        "    if step % refresh_target_network_freq == 0:\n",
        "        # Load agent weights into target_network\n",
        "        # <YOUR CODE>\n",
        "        target_network.load_state_dict(agent.state_dict())\n",
        "\n",
        "    if step % eval_freq == 0:\n",
        "        mean_rw_history.append(evaluate(\n",
        "            make_env(clip_rewards=True, seed=step), agent, n_games=3 * n_lives, greedy=True)\n",
        "        )\n",
        "        initial_state_q_values = agent.get_qvalues(\n",
        "            [make_env(seed=step).reset()]\n",
        "        )\n",
        "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
        "\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" %\n",
        "              (len(exp_replay), agent.epsilon))\n",
        "\n",
        "        plt.figure(figsize=[16, 9])\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.title(\"Mean reward per life\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "\n",
        "        assert not np.isnan(td_loss_history[-1])\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.title(\"TD loss history (smoothened)\")\n",
        "        plt.plot(utils.smoothen(td_loss_history))\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.title(\"Initial state V\")\n",
        "        plt.plot(initial_state_v_history)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.title(\"Grad norm history (smoothened)\")\n",
        "        plt.plot(utils.smoothen(grad_norm_history))\n",
        "        plt.grid()\n",
        "    \n",
        "        plt.show()\n",
        "    if step % save_freq == 0 :#and step > 0:\n",
        "        torch.save({\n",
        "                    \"agent_state_dict\": agent.state_dict(),\n",
        "                    \"opt_state_dict\": opt.state_dict(),\n",
        "                    \"env\": env,\n",
        "                    \"step\": step,\n",
        "                    \"exp_replay\": exp_replay,\n",
        "                    \"mean_rw_history\": mean_rw_history,\n",
        "                    \"td_loss_history\": td_loss_history,\n",
        "                    \"grad_norm_history\": grad_norm_history,\n",
        "                    \"initial_state_v_history\": initial_state_v_history\n",
        "                      }, PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 321/1800000 [00:10<16:36:54, 30.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b270169abe25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# play\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_and_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e5cfb39b2d3b>\u001b[0m in \u001b[0;36mplay_and_record\u001b[0;34m(initial_state, agent, env, exp_replay, n_steps)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msum_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/framebuffer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m\"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnew_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframebuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f4af0664d84a>\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresult_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mresult_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_gray_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mresult_img\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f4af0664d84a>\u001b[0m in \u001b[0;36m_to_gray_scale\u001b[0;34m(self, rgb, channel_weights)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mgray_scale_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgray_scale_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdjytLttIPx-"
      },
      "source": [
        "Agent is evaluated for 1 life, not for a whole episode of 5 lives. Rewards in evaluation are also truncated. Cuz this is what environment the agent is learning in and in this way mean rewards per life can be compared with initial state value\n",
        "\n",
        "**The goal is to get 10 points in the real env**. So 3 or more points in the preprocessed one will probably be enough. You can interrupt learning then."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L5pVzQNIPx-"
      },
      "source": [
        "Final scoring is done on a whole episode with all 5 lives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dyQQpjIPx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860c2914-6965-4fd9-f031-b6847258628e"
      },
      "source": [
        "final_score = evaluate(\n",
        "  make_env(clip_rewards=False, seed=9),\n",
        "    agent, n_games=30, greedy=True, t_max=10 * 1000\n",
        ") * n_lives\n",
        "print('final score:', final_score)\n",
        "assert final_score >= 10, 'not as cool as DQN can'\n",
        "print('Cool!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 321/1800000 [00:29<16:36:54, 30.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "final score: 25.0\n",
            "Cool!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_LyF_xZIPx_"
      },
      "source": [
        "## How to interpret plots:\n",
        "\n",
        "This aint no supervised learning so don't expect anything to improve monotonously. \n",
        "* **TD loss** is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
        "* **grad norm** just shows the intensivity of training. Not ok is growing to values of about 100 (or maybe even 50) though it depends on network architecture.\n",
        "* **mean reward** is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
        " * In basic q-learning implementation it takes about 40k steps to \"warm up\" agent before it starts to get better.\n",
        "* **Initial state V** is the expected discounted reward for episode in the oppinion of the agent. It should behave more smoothly than **mean reward**. It should get higher over time but sometimes can experience drawdowns because of the agaent's overestimates.\n",
        "* **buffer size** - this one is simple. It should go up and cap at max size.\n",
        "* **epsilon** - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - it means you need to increase epsilon. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
        "* Smoothing of plots is done with a gaussian kernel\n",
        "\n",
        "At first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n",
        "\n",
        "**Training will take time.** A lot of it actually. Probably you will not see any improvment during first **150k** time steps (note that by default in this notebook agent is evaluated every 5000 time steps).\n",
        "\n",
        "But hey, long training time isn't _that_ bad:\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WmR34RmIPx_"
      },
      "source": [
        "## About hyperparameters:\n",
        "\n",
        "The task has something in common with supervised learning: loss is optimized through the buffer (instead of Train dataset). But the distribution of states and actions in the buffer **is not stationary** and depends on the policy that generated it. It can even happen that the mean TD error across the buffer is very low but the performance is extremely poor (imagine the agent collecting data to the buffer always manages to avoid the ball).\n",
        "\n",
        "* Total timesteps and training time: It seems to be so huge, but actually it is normal for RL.\n",
        "\n",
        "* $\\epsilon$ decay shedule was taken from the original paper and is like traditional for epsilon-greedy policies. At the beginning of the training the agent's greedy policy is poor so many random actions should be taken.\n",
        "\n",
        "* Optimizer: In the original paper RMSProp was used (they did not have Adam in 2013) and it can work not worse than Adam. For us Adam was default and it worked.\n",
        "\n",
        "* lr: $10^{-3}$ would probably be too huge\n",
        "\n",
        "* batch size: This one can be very important: if it is too small the agent can fail to learn. Huge batch takes more time to process. If batch of size 8 can not be processed on the hardware you use take 2 (or even 4) batches of size 4, divide the loss on them by 2 (or 4) and make optimization step after both backward() calls in torch.\n",
        "\n",
        "* target network update frequency: has something in common with learning rate. Too frequent updates can lead to divergence. Too rare can lead to slow leraning. For millions of total timesteps thousands of inner steps seem ok. One iteration of target network updating is an iteration of the (this time approximate) $\\gamma$-compression that stands behind Q-learning. The more inner steps it makes the more accurate is the compression.\n",
        "* max_grad_norm - just huge enough. In torch clip_grad_norm also evaluates the norm before clipping and it can be convenient for logging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXUVUhwpIPx_"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsmUhvHUIPx_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "62282e7c-154c-4f9a-bbb3-0d44d6d87b92"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [evaluate(env_monitor, agent, n_games=n_lives, greedy=True) for _ in range(10)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d4f9f50b3a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_lives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-d4f9f50b3a76>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_lives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-93ef57a33c63>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(env, agent, n_games, greedy, t_max)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/framebuffer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m\"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnew_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframebuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f4af0664d84a>\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresult_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mresult_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_gray_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mresult_img\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f4af0664d84a>\u001b[0m in \u001b[0;36m_to_gray_scale\u001b[0;34m(self, rgb, channel_weights)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mgray_scale_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgray_scale_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z1rWyoUIPx_"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnA2PYVaIPx_"
      },
      "source": [
        "## Submit to Coursera"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TvQykqYIPx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1ad6f6-7ab4-4d33-8479-edc0991f5130"
      },
      "source": [
        "from submit import submit_breakout\n",
        "env = make_env()\n",
        "submit_breakout(agent, env, evaluate, 'caovi456@gmail.com', '3ov6xaysqS2H0ii4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your average reward is 3.06 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SpA58NeIPyA"
      },
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Let's have a closer look at this.\n",
        "\n",
        "If average episode score is below 200 using all 5 lives, then probably DQN has not converged fully. But anyway let's make a more complete record of an episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpqf0VAzIPyA"
      },
      "source": [
        "eval_env = make_env(clip_rewards=False)\n",
        "record = utils.play_and_log_episode(eval_env, agent)\n",
        "print('total reward for life:', np.sum(record['rewards']))\n",
        "for key in record:\n",
        "    print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyrqMXH3IPyA"
      },
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(record['v_mc'], record['v_agent'])\n",
        "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
        "       'black', linestyle='--', label='x=y')\n",
        "\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_title('State Value Estimates')\n",
        "ax.set_xlabel('Monte-Carlo')\n",
        "ax.set_ylabel('Agent')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaPZ1hc5IPyA"
      },
      "source": [
        "$\\hat V_{Monte-Carlo}(s_t) = \\sum_{\\tau=0}^{episode~end} \\gamma^{\\tau-t}r_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWaCALsBIPyB"
      },
      "source": [
        "Is there a big bias? It's ok, anyway it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZSihF5EIPyB"
      },
      "source": [
        "## More\n",
        "\n",
        "If you want to play with DQN a bit more, here's a list of things you can try with it:\n",
        "\n",
        "### Easy:\n",
        "* Implementing __double q-learning__ shouldn't be a problem if you've already have target networks in place.\n",
        "  * You will probably need `tf.argmax` to select best actions\n",
        "  * Here's an original [article](https://arxiv.org/abs/1509.06461)\n",
        "\n",
        "* __Dueling__ architecture is also quite straightforward if you have standard DQN.\n",
        "  * You will need to change network architecture, namely the q-values layer\n",
        "  * It must now contain two heads: V(s) and A(s,a), both dense layers\n",
        "  * You should then add them up via elemwise sum layer.\n",
        "  * Here's an [article](https://arxiv.org/pdf/1511.06581.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDMYbgFrIPyB"
      },
      "source": [
        "### Hard: Prioritized experience replay\n",
        "\n",
        "In this section, you're invited to implement prioritized experience replay\n",
        "\n",
        "* You will probably need to provide a custom data structure\n",
        "* Once pool.update is called, collect the pool.experience_replay.observations, actions, rewards and is_alive and store them in your data structure\n",
        "* You can now sample such transitions in proportion to the error (see [article](https://arxiv.org/abs/1511.05952)) for training.\n",
        "\n",
        "It's probably more convenient to explicitly declare inputs for \"sample observations\", \"sample actions\" and so on to plug them into q-learning.\n",
        "\n",
        "Prioritized (and even normal) experience replay should greatly reduce amount of game sessions you need to play in order to achieve good performance. \n",
        "\n",
        "While it's effect on runtime is limited for atari, more complicated envs (further in the course) will certainly benefit for it.\n",
        "\n",
        "There is even more out there - see this [overview article](https://arxiv.org/abs/1710.02298)."
      ]
    }
  ]
}
